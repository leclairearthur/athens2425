{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es_iHEzxW190"
   },
   "source": [
    "# Practical Work 6: Classification of MNIST digits with a neural network\n",
    "\n",
    "Practical work originally created by Alasdair Newson (https://sites.google.com/site/alasdairnewson/)\n",
    "\n",
    "and later modified by Loïc Le Folgoc.\n",
    "\n",
    "### Goal:\n",
    "\n",
    "We want to implement a Convolutional Neural Network (CNN) for image recognition on the dataset MNIST (images of digits).\n",
    "\n",
    "<br>We will first code the simple ConvNet described below using the Pytorch environment : https://pytorch.org/.\n",
    "\n",
    "- The input of the CNN is a set of (3,m,n) image tensors (m and n depend on the dataset).\n",
    "- We apply\n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same' (i.e. we apply zero-padding)\n",
    "    - additive biases\n",
    "    - a ReLu activation function\n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same' (i.e. we apply zero-padding)\n",
    "    - additive biases\n",
    "    - a ReLu activation function\n",
    "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    - We then Flatten the data (reduce them to a vector in order to be able to apply a Fully-Connected layer to it)\n",
    "    - A softmax activation function which outputs are the $P(y_c | X)$ (multi-class problem)\n",
    "\n",
    "    \n",
    "For convolutional layers, we will use the border conditions \"SAME\".\n",
    "    \n",
    "### Your task:\n",
    "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cgyu2GBVW192"
   },
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1Qj5KY79W192"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHJ17JDiliHA"
   },
   "source": [
    "### CNN model in Pytorch\n",
    "\n",
    "There are several ways to write a CNN model in Pytorch. In this lab, you will be using the _Sequential_ class of Pytorch (similarly to Tensorflow). We will see the syntax further on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_S76Wi_W199"
   },
   "source": [
    "# Import data\n",
    "\n",
    "We first import the MNIST dataset. The training set is imported in `mnist_trainset` and the test set in `mnist_testset`.\n",
    "\n",
    "In practice, training on `mnist_trainset` takes too much time for this practical work. For this reason, we define a smaller training set (`mnist_trainset_reduced`) with a random subset of images. We will use `mnist_trainset_reduced` when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BrYw9LK9W19-",
    "outputId": "70171747-fa96-4254-85a7-c32db59908d1"
   },
   "outputs": [],
   "source": [
    "# Convert input to Pytorch tensors (ToTensor includes a rescaling from the range [0,255] to [0.0,1.0])\n",
    "input_transform=transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download MNIST training data\n",
    "mnist_trainset = datasets.MNIST(root='./data',train=True,download=True,transform=input_transform)\n",
    "print(mnist_trainset)\n",
    "\n",
    "# Download test dataset\n",
    "mnist_testset = datasets.MNIST(root='./data',train=False,download=True,transform=input_transform)\n",
    "\n",
    "# Create data loader with smaller dataset size\n",
    "max_mnist_size = 2000\n",
    "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EqPkZg4OYR5"
   },
   "source": [
    "We also make a direct access to the training and test data as `torch` tensors. We will use them for visualization purposes and to compute the final training/test accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXXsOChGKMO1"
   },
   "outputs": [],
   "source": [
    "# Extract the actual data and labels\n",
    "X_train = torch.unsqueeze(mnist_trainset.data,axis=1)[mnist_trainset_reduced.indices]/255.0\n",
    "Y_train = mnist_trainset.targets[mnist_trainset_reduced.indices]\n",
    "X_test = torch.unsqueeze(mnist_testset.data,axis=1)/255.0\n",
    "Y_test = mnist_testset.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ss6fBjWrAS4U"
   },
   "source": [
    "## Exploring the data\n",
    "\n",
    "We can explore the dataset mnist_trainset manually, although when we train the model, we will use the ```DataLoader``` of Pytorch (see later).\n",
    "\n",
    "The images are contained in a sub-structure of ```mnist_trainset``` called ```data```. The labels are contained in another sub-structure of ```mnist_trainset``` called ```targets```. Note that these are kept in their native format (the transformations are not applied to them), so to use them we have to apply the transformation manually, as above.\n",
    "\n",
    "__NOTE__ In general, if you want to find out what a structure contains, use the command ```dir()```, this will give you a list of the sub-structures.\n",
    "\n",
    "__NOTE__ `mnist_trainset_reduced` is a `Subset` object rather than a `Dataset` object. We cannot call `.data` and `.target` directly on it, although we can pass it as argument to a `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rMqnFhbH9bcq",
    "outputId": "62a9bcc7-2ed5-4059-e27c-d12c8d180b11"
   },
   "outputs": [],
   "source": [
    "print(dir(mnist_trainset))\n",
    "\n",
    "print(\"Size of training data : \", mnist_trainset.data.shape)\n",
    "print(\"Size of training labels : \", mnist_trainset.targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnTkYmu-W1-E"
   },
   "source": [
    "The mnist dataset has 10 classes. These are the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvJ7McU7W1-F"
   },
   "outputs": [],
   "source": [
    "mnist_list = [ '0', '1','2','3','4','5','6','7','8','9']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f_7d1NnW1-L"
   },
   "source": [
    "## Display some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "9OcnfCwbW1-M",
    "outputId": "67da9005-6909-4b51-f0d6-5e15450f0ea5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for idx in range(0,10):\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    rand_ind = np.random.randint(0,mnist_trainset.data.shape[0])\n",
    "    plt.imshow(mnist_trainset.data[rand_ind,:,:],cmap='gray')\n",
    "    plt.title(mnist_list[int(mnist_trainset.targets[rand_ind])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzRfY8QTBIX-"
   },
   "source": [
    "## Defining the model for MNIST\n",
    "\n",
    "We will now define the simple CNN described above, for use with MNIST. The input of the CNN is a set of (28,28,1) image tensors. We apply the following layers:\n",
    "\n",
    "- a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
    "- a ReLu activation function\n",
    "    \n",
    "- a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same'\n",
    "- a ReLu activation function\n",
    "- a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    \n",
    "- We then Flatten the data: reduce them to a vector in order to be able to apply a Fully-Connected layer to it\n",
    "- Dense (fully connected) layer. Note, you will have to determine the input size, that is to say the number of elements after the last Max Pooling layer.\n",
    "\n",
    "__VERY IMPORTANT NOTE !!!__\n",
    "\n",
    "Pytorch carries out the softmax which we would expect at the end of our network automatically in the loss function that we will use, so there is no need to add it. Nevertheless, you must understand that the network output is a vector (of logits) which is _not_ normalised to be a probability distribution. This will be important later on.\n",
    "\n",
    "Now, we define the following hyper-parameters of the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T9d8TYFBONz"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "batch_size = 64\n",
    "nb_classes = int(mnist_trainset.targets.max()+1)\n",
    "\n",
    "nb_filters = 32       # number of convolutional filters to use\n",
    "kernel_size = (3, 3)  # convolution kernel size\n",
    "pool_size = (2, 2)    # size of pooling area for max pooling\n",
    "\n",
    "# --- Size of the successive layers\n",
    "n_h_0 = 1             # greyscale input images\n",
    "n_h_1 = nb_filters\n",
    "n_h_2 = nb_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MOihxZ-W1-W"
   },
   "source": [
    "## Defining a CNN with the Sequential API of Pytorch for MNIST\n",
    "\n",
    "We are now going to create the CNN with Pytorch.\n",
    "\n",
    "The Sequential approach is quite similar to that of Tensorflow. To define a model, just write:\n",
    "\n",
    "```my_model = torch.nn.Sequential( first_layer, second_layer, ...)```\n",
    "\n",
    "Your work here is to try understanding the following lines, and to draw the architecture of the neural network they define.\n",
    "\n",
    "You can use the documentation of Pytorch to understand the parameters used in these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npx-4C1SW1-X"
   },
   "outputs": [],
   "source": [
    "mnist_model = torch.nn.Sequential(\n",
    "        torch.nn.Conv2d(n_h_0,n_h_1,kernel_size=kernel_size,stride=1,padding='same'),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Conv2d(n_h_1,n_h_2,kernel_size=kernel_size,stride=1,padding='same'),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool2d(kernel_size=pool_size,stride=pool_size),\n",
    "        torch.nn.Flatten(),\n",
    "        torch.nn.Linear(14*14*n_h_2,nb_classes),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K0m7iDFLUJd"
   },
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAiB6Vy3LaCC"
   },
   "source": [
    "We use the ```torch.utils.data.DataLoader``` function of Pytorch to easily iterate over mini-batches of data. ```torch.utils.data.DataLoader``` is a useful function to extract batches of data from a dataset, applying the transformations which we have specified (conversion to Pytorch tensor, normalisation etc).\n",
    "\n",
    "We will train using the smaller training set, `mnist_trainset_reduced`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vju5hEHYLX0U"
   },
   "outputs": [],
   "source": [
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset_reduced, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FJS2SGeEwHF"
   },
   "source": [
    "## Define loss function and optimiser\n",
    "\n",
    "Pytorch provides an easy way to define the loss criterion to optimise. The syntax is (considering that the Adam optimiser is used):\n",
    "\n",
    "- ```criterion = torch.nn.BCELoss()``` or ```criterion = torch.nn.CrossEntropyLoss()```, etc., depending on your problem.\n",
    "- ```optimizer = torch.optim.Adam(mnist_model.parameters(), lr=learning_rate)```\n",
    "\n",
    "Fill in the following code, choosing the correct criterion to optimise. For the criterion, the individual loss over individual data samples can be aggregated into the total loss in several ways. Choose `reduction='sum'`, which takes the sum of individual losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AK1pxqFCE090"
   },
   "outputs": [],
   "source": [
    "# BEGIN STUDENT CODE\n",
    "# ...\n",
    "# END STUDENT CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42zy2XwsNfTQ"
   },
   "source": [
    "## CNN prediction conversion\n",
    "\n",
    "We recall here that the output of the classification CNN in Pytorch is a vector which is __NOT__ normalised to be a probability distribution. Therefore, for the purposes of finding the prediction of the CNN, we create a function which first converts an input vector to a probability distribution, and then determines the most likely class for each vector. The output should be, for each vector, an integer between 0 and (number of classes) $-1$.\n",
    "\n",
    "The inputs to this function will be Pytorch tensors, so you can use the following Pytorch functions on them :\n",
    "\n",
    "- ```torch.nn.Softmax()```\n",
    "- ```torch.argmax()```\n",
    "\n",
    "Create this function now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqCPink-N1LB"
   },
   "outputs": [],
   "source": [
    "def vector_to_class(x):\n",
    "  # BEGIN STUDENT CODE\n",
    "  # ... #\n",
    "  # END STUDENT CODE\n",
    "  return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYzRpoTgGhpG"
   },
   "source": [
    "## Accuracy\n",
    "\n",
    "Now, define a function which calculates the accuracy of the output of the neural network, with respect to the input labels. We consider that the input is a vector of class numbers (similar to the output of `vector_to_class`, but converted to a numpy array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4V5LKDhGsQT"
   },
   "outputs": [],
   "source": [
    "def cnn_accuracy(predict,labels):\n",
    "  # BEGIN STUDENT CODE\n",
    "  # ...\n",
    "  # END STUDENT CODE\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljHi0tfiW1-h"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Now, we carry out the actual training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVe9ZWAdW1-h",
    "outputId": "15fccac4-2907-4845-841a-662ce0f4d7e5"
   },
   "outputs": [],
   "source": [
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "for epoch in range(0,n_epochs):\n",
    "  train_loss=0.0\n",
    "  all_labels = []\n",
    "  all_predicted = []\n",
    "\n",
    "  for batch_idx, (imgs, labels) in enumerate(mnist_train_loader):\n",
    "    # pass the samples through the network\n",
    "    # ...\n",
    "    # apply loss function\n",
    "    # ...\n",
    "    # set the gradients back to 0\n",
    "    # ...\n",
    "    # backpropagation\n",
    "    # ...\n",
    "    # parameter update\n",
    "    # ...\n",
    "    # compute the train loss\n",
    "    train_loss.append(loss.item())\n",
    "    # store labels and class predictions\n",
    "    all_labels.extend(labels.tolist())\n",
    "    all_predicted.extend(vector_to_class(predict).tolist())\n",
    "\n",
    "  print('Epoch:{} Train Loss:{:.4f}'.format(epoch,train_loss/len(mnist_train_loader.dataset)))\n",
    "\n",
    "  # calculate accuracy\n",
    "  print('Accuracy:{:.4f}'.format(cnn_accuracy(np.array(all_predicted),np.array(all_labels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tX5vvhNgDOGh"
   },
   "source": [
    "<br>Let's compute the final training and test accuracies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rOsF40hkEqx1",
    "outputId": "ffc8d3c8-1ef1-4a4c-b082-777be879e675"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate accuracy on the training set and the test set\n",
    "\n",
    "# BEGIN FILL IN STUDENT (use X_train, Y_train, X_test, Y_test)\n",
    "# ...\n",
    "# END FILL IN STUDENT\n",
    "\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "id": "cRxCNvMO5Yzm",
    "outputId": "3d78960f-dcf8-4a3d-d7be-d1fce3d98e83"
   },
   "outputs": [],
   "source": [
    "print(\"Visual results : \")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx in range(0,10):\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    rand_ind = np.random.randint(0,X_test.shape[0])\n",
    "    test_img = torch.unsqueeze(X_test[rand_ind,:,:,:],axis=1)\n",
    "    predicted_class = vector_to_class(mnist_model(test_img))\n",
    "    plt.imshow(test_img.squeeze(),cmap='gray')\n",
    "    plt.title(mnist_list[int(predicted_class)])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
